CodeBook.md
# Course Project, Getting and Cleaning Data
***
## Project Description
This project prepares a tidy data set, from a specific set of input data that contains smartphone accelerometer data from 30 individuals.  

**author**: Neil Strange  

**date**: 21.Sep.2015  

**input**: A subdirectory of source data: "UCI HAR Dataset" should be in the current working directory, containing accelerometer data from a number of subjects. A zip of this data is available from https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip.  

**output**: One tidy data file is produced - for each subject and activity combination a list of the mean of each measurement.  

## Package Dependencies
This script loads and uses the following additional packages:  
* dplyr, 0.4.3 or later
* data.table, 1.9.4 or later 
* reshape2, 1.4.1 or later  

## Interpretation of 'Tidy Data' for this Project
Data is considered tidy if it meets a number of criteria. 

* *Columns are named and the names are meaningful*: the data is scientific, the measurement names provided in the features file are appropriate for scientific use and are more meaningful than the original column names V1, V2... V561.  

* *Coded Values are Substituted for codes*: where activity codes 1..6 were used these are replaced with the corresponding strings set out in the activity_labels file, where 1 = WALKING, 2 = WALKING_UPSTAIRS, etc.  

* *There is one measure per row*: the original data contains multiple measure columns (561 columns of measures, reduced to 85 of interest), these can be reduced to two columns: variable and value, note that - in order not to lose the fact that a number of measures were made at the same time, an extra column is added 'observation number' before melting the data set to retain the linkage between different observed measures.

## Data Processing
### Raw Source Data
Source data is contained in a directory structure which should be in the script's working directory when launched. This directory should have the name: "UCI HAR Dataset".  

The structure includes a number of subdirectories and files:  
* *\test* - a directory containing the test data set (see below)
* *\train* - a directory containing the training data set (see below)
* *activity_labels.txt* - a list of labels used to describe activities, activity data is stored in the test/training data sets using an integer number (1-6), this file contains the corresponding lookup values, i.e. a value of 1 is a 'WALKING' activity. The assignment asks these labels to be substituted for the integers in the target tidy data set.
* *features.txt* - a list of 561 measurements made by the sensor
* *features_info.txt* - a description of how the measurements were made and how they are structured
* *README.txt* - a description of the experimental source of the data, how measurements were made, the directories and files that comprise the data set and information about licensing  
Measurements were made for a number of subjects. The measures were divided into two sets - a test data set and a training data set, each set has its own subdirectory - \test and \train, and they contain further subdirectories and files. The \test and \train directories have identical structures, the table below describes the \test directory contents, however it is applicable to \train as well, just substitute file names using '_test' with '_train':  
* X_test - contains measurements, one row per measurement event, each row with 561 observations labelled V1, V2, ...V561
* y_test - contains information about activities performed, as integers 1-6 (see features.txt to decode these), there is one entry in y_test for each entry in X_test and these correspond - so the first entry in y_test is the activity underway for the first measurement set in X_test
* subject_test - contains information about the subject being measured, identified as an integer, there is one entry in subject_test for each entry in X_test and these correspond - so the first entry in subject_test is the subject of the first measurement set in X_test
* \Inertial Signals - low-level body accelerator and body gyro data over a number of files, this is too low a level for use in this project and can be ignored  

### Target Data

### Processing Steps
The script contains a number of steps:  

**step 1 - initial preparation**
* This step verifies the working directory contains the required data set and stops with an error message if not.
* It saves the current working directory so it can be reset on exit.
* It sets some variables that store the subdirectory location for the test, train and features data sets.

**step 2 - read in the X_test/train data files**  
* Read in the files as comma separated data sets.
* Each is located in its own subdirectory so use the pre-set subdirectory values to locate the files.
* Keep them in separate data frames for processing before they are merged.
* The data frames are x_test and x_train.

**step 3 - rename the x_test/x_train data frame columns to use more readable names**  
* The raw data uses column names 'V1', V2', ... 'V561'. These do not explain what the columns represent.
* The features file contains a mapping of column names suitable for an engineer to use, e.g. tBodyAcc-mean-X, meaning a measure of mean body acceleration in the X direction measured over a time frame.
* Replace the column names with the set of names taken from the features.txt file.
* Read in the features file as a vector, storing what will become the new column names.
* Pre-process the features vector to remove (), as these are not suitable for column names, also convert '-' and ',' characters to '_'.
* Use the setnames() function to set both the x_test and x_train data frame column names to the features vector.

**step 4 - append the y_test/y_train activity data**  
* The y_test and Y_train files contain information about the activities being performed when the X-test and x_train measures were taken.
* The y_test and y_train data sets contain the same number of rows as the x_test and x_train data sets, each row in y_test and y_train corresponds to the same number row in x_test and x_train.
* Read the y_test and y_train data sets into data frames.
* Each is located in its own subdirectory so use the pre-set subdirectory values to locate the files.
* The raw activity data uses a numeric code 1:6.
* There are textual explanations of each code (in the activity_labels.txt file), 1 = WALKING, 2 = WALKING_UPSTAIRS, etc.
* Replace the raw activity codes with the activity labels to make them readable:
    + assemble the code explanations in its own vector - activity_code
    + loop through y_test and y_train replacing each code with its explanation value
    + hold the results in new vectors y_test_activities and y_train_activities
* Create new columns in x_test and x_train by column binding the y_test_activities and y_train_activities data
* name the new columns 'Activity'

**step 5 - append the subject_test and subject_train data**
* Create a new column in X_% and column bind the subject_% data.

**step 6 - merge the test and training data sets**
* Merge the data sets into X_data holding the combined set.

**step 7 drop any non-mean or non-standard deviation measures**
* Build a logical vector that indicates if a feature includes the text fragments 'mean' or 'std' use the logical vector to drop any columns we don't need, note that columns 1-2 contain subject and activity data and are also retained. This reduces the 561 measurement columns down to 87.

**step 8 - melt the data set, reduce 87 measurement columns to a variable and value column**
* Each row contains 87 measures plus a subject and activity column. One row is in place for each measurement. To retain the linkage between a measurement and the 87 measures prepend an index to the data set before melting, this index identifies the particular measurement, ensuring it isn't lost, and allows original data to be reconstructed.

**step 9 - calculate means for each measure by subject and activity**
* 

**step 10 - print the tidy data set** 
* Print the results to a file "tidy_data.txt" using write.table()
* Use the row.name = FALSE setting

**step 11 - restore the environment**
* Reset the environment to where it was when the script started.


## Description of Variables

tBodyAcc-mean-X
tBodyAcc-mean-Y
tBodyAcc-mean-Z
tBodyAcc-std-X
tBodyAcc-std-Y
tBodyAcc-std-Z

tGravityAcc-mean-X
tGravityAcc-mean-Y
tGravityAcc-mean-Z
tGravityAcc-std-X
tGravityAcc-std-Y
tGravityAcc-std-Z

tBodyAccJerk-mean-X
tBodyAccJerk-mean-Y
tBodyAccJerk-mean-Z
tBodyAccJerk-std-X
tBodyAccJerk-std-Y
tBodyAccJerk-std-Z

tBodyGyro-mean-X
tBodyGyro-mean-Y
tBodyGyro-mean-Z
tBodyGyro-std-X
tBodyGyro-std-Y
tBodyGyro-std-Z

tBodyGyroJerk-mean-X
tBodyGyroJerk-mean-Y
tBodyGyroJerk-mean-Z
tBodyGyroJerk-std-X
tBodyGyroJerk-std-Y
tBodyGyroJerk-std-Z

tBodyAccMag-mean
tBodyAccMag-std
tGravityAccMag-mean
tGravityAccMag-std

tBodyAccJerkMag-mean
tBodyAccJerkMag-std

tBodyGyroMag-mean
tBodyGyroMag-std

tBodyGyroJerkMag-mean
tBodyGyroJerkMag-std

fBodyAcc-mean-X
fBodyAcc-mean-Y
fBodyAcc-mean-Z
fBodyAcc-std-X
fBodyAcc-std-Y
fBodyAcc-std-Z

fBodyAcc-meanFreq-X
fBodyAcc-meanFreq-Y
fBodyAcc-meanFreq-Z

fBodyAccJerk-mean-X
fBodyAccJerk-mean-Y
fBodyAccJerk-mean-Z

fBodyAccJerk-std-X
fBodyAccJerk-std-Y
fBodyAccJerk-std-Z
fBodyAccJerk-meanFreq-X
fBodyAccJerk-meanFreq-Y
fBodyAccJerk-meanFreq-Z

fBodyGyro-mean-X
fBodyGyro-mean-Y
fBodyGyro-mean-Z
fBodyGyro-std-X
fBodyGyro-std-Y
fBodyGyro-std-Z
fBodyGyro-meanFreq-X
fBodyGyro-meanFreq-Y
fBodyGyro-meanFreq-Z
fBodyAccMag-mean
fBodyAccMag-std
fBodyAccMag-meanFreq
fBodyBodyAccJerkMag-mean
fBodyBodyAccJerkMag-std
fBodyBodyAccJerkMag-meanFreq
fBodyBodyGyroMag-mean
fBodyBodyGyroMag-std
fBodyBodyGyroMag-meanFreq
fBodyBodyGyroJerkMag-mean
fBodyBodyGyroJerkMag-std
fBodyBodyGyroJerkMag-meanFreq
angletBodyAccJerkMean,gravityMean
angletBodyGyroMean,gravityMean
angletBodyGyroJerkMean,gravityMean
angleX,gravityMean
angleY,gravityMean
angleZ,gravityMean

## Sources

1. Tidy Data, Hadley Wickham, Journal of Statistical Software




