CodeBook.md
# Course Project, Getting and Cleaning Data
***
## Project Description
This project prepares a tidy data set, from a specific set of input data that contains smartphone accelerometer data from 30 individuals.  

**author**: Neil Strange  

**date**: 21.Sep.2015  

**input**: A subdirectory of source data: "UCI HAR Dataset" should be in the current working directory, containing accelerometer data from a number of subjects. A zip of this data is available from https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip.  

**output**: One tidy data file is produced - for each subject and activity combination a list of the mean of each measurement.  

## Package Dependencies
This script loads and uses the following additional packages:  
* dplyr, 0.4.3 or later
* data.table, 1.9.4 or later 
* reshape2, 1.4.1 or later  

## Interpretation of 'Tidy Data' for this Project
Data is considered tidy if it meets a number of criteria. 

* *Columns are named and the names are meaningful*: the data is scientific, the measurement names provided in the features file are appropriate for scientific use and are more meaningful than the original column names V1, V2... V561.  

* *Coded Values are Substituted for codes*: where activity codes 1..6 were used these are replaced with the corresponding strings set out in the activity_labels file, where 1 = WALKING, 2 = WALKING_UPSTAIRS, etc.  

* *There is one measure per row*: the original data contains multiple measure columns (561 columns of measures, reduced to 85 of interest), these can be reduced to two columns: variable and value, note that - in order not to lose the fact that a number of measures were made at the same time, an extra column is added 'observation number' before melting the data set to retain the linkage between different observed measures.

## Data Processing
### Raw Source Data
Source data is contained in a directory structure which should be in the script's working directory when launched. This directory should have the name: "UCI HAR Dataset".  

The structure includes a number of subdirectories and files:  
* *\test* - a directory containing the test data set (see below)
* *\train* - a directory containing the training data set (see below)
* *activity_labels.txt* - a list of labels used to describe activities, activity data is stored in the test/training data sets using an integer number (1-6), this file contains the corresponding lookup values, i.e. a value of 1 is a 'WALKING' activity. The assignment asks these labels to be substituted for the integers in the target tidy data set.
* *features.txt* - a list of 561 measurements made by the sensor
* *features_info.txt* - a description of how the measurements were made and how they are structured
* *README.txt* - a description of the experimental source of the data, how measurements were made, the directories and files that comprise the data set and information about licensing 

Measurements were made for a number of subjects. The measures were divided into two sets - a test data set and a training data set, each set has its own subdirectory - \test and \train, and they contain further subdirectories and files. The \test and \train directories have identical structures, the table below describes the \test directory contents, however it is applicable to \train as well, just substitute file names using '_test' with '_train':  
* *X_test* - contains measurements or calculations based on these measurements, one row per observation event, each row with 561 observations labelled V1, V2, ...V561
* *y_test* - contains information about activities performed, as integers 1-6 (see features.txt to decode these), there is one entry in y_test for each entry in X_test and these correspond - so the first entry in y_test is the activity underway for the first measurement set in X_test
* *subject_test* - contains information about the subject being measured, identified as an integer, there is one entry in subject_test for each entry in X_test and these correspond - so the first entry in subject_test is the subject of the first measurement set in X_test
* *\Inertial Signals* - a subdirectory holding low-level body accelerator and body gyro data over a number of files, this is too low a level for use in this project and can be ignored  

### Target Data
There are two 
* x_data_melt - this is a narrow tidy data set holding data for all mean and standard deviation measurements
* x_cast - is a summary table holding the mean of each measurement's values for each subject and activity combination, x_cast is printed by the script as a tidy_data.txt file

**x_data_melt**
A data frame, 5 columns wide, with 885,174 rows
The columns are:
* observation_number: integer, the row number from the original merged training and test data set, it ties together each of the 86 measurements made in a single observation
* Subject: integer, in the range 1:30, identifying the person who was the subject of the measurement
* Activity, character, one of 6 values (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)
* variable: character, the name of a variable, one of 86 measurements (see below for the list of variables)
* value: numeric, the value of that variable, all values are in the range -1 to +1, having been normalised

**x_cast**
A data frame, 88 columns wide, 180 rows (all combinations of 6 activities x 30 subjects)
* Subject: integer, in the range 1:30, identifying the person who was the subject of the measurement
* Activity, character, one of 6 values (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)
* Columns 3:88, numeric, each column contains data about one variable, the data is the mean of all values for that variable for each subject and activity combination

## Processing Steps
The script contains a number of steps:  

**step 1 - initial preparation**
* This step verifies the working directory contains the required data set and stops with an error message if not.
* It saves the current working directory so it can be reset on exit.
* It sets some variables that store the subdirectory location for the test, train and features data sets.

**step 2 - read in the X_test/train data files**  
* Read in the files as comma separated data sets.
* Each is located in its own subdirectory so use the pre-set subdirectory values to locate the files.
* Keep them in separate data frames for processing before they are merged.
* The data frames are x_test and x_train.

**step 3 - rename the x_test/x_train data frame columns to use more readable names**  
* The raw data uses column names 'V1', V2', ... 'V561'. These do not explain what the columns represent.
* The features file contains a mapping of column names suitable for an engineer to use, e.g. tBodyAcc-mean-X, meaning a measure of mean body acceleration in the X direction measured over a time frame.
* Replace the column names with the set of names taken from the features.txt file.
* Read in the features file as a vector, storing what will become the new column names.
* Pre-process the features vector to remove (), as these are not suitable for column names, also convert '-' and ',' characters to '_'.
* Use the setnames() function to set both the x_test and x_train data frame column names to the features vector.

**step 4 - append the y_test/y_train activity data and relace codes with activity descriptions**  
* The y_test and Y_train files contain information about the activities being performed when the X-test and x_train measures were taken.
* The y_test and y_train data sets contain the same number of rows as the x_test and x_train data sets, each row in y_test and y_train corresponds to the same number row in x_test and x_train.
* Read the y_test and y_train data sets into data frames.
* Each is located in its own subdirectory so use the pre-set subdirectory values to locate the files.
* The raw activity data uses a numeric code 1:6.
* There are textual explanations of each code (in the activity_labels.txt file), 1 = WALKING, 2 = WALKING_UPSTAIRS, etc.
* Replace the raw activity codes with the activity labels to make them readable:
    + assemble the code explanations in its own vector - activity_code
    + loop through y_test and y_train replacing each code with its explanation value
    + hold the results in new vectors y_test_activities and y_train_activities
* Create new columns in x_test and x_train by column binding the y_test_activities and y_train_activities data
* name the new columns 'Activity'

**step 5 - append the subject_test and subject_train data**
* The subject_test and subject_train files contain information about subjects(people) involved in the experiment.
* Subjects are numbered 1:30, we don't know any more about them from the data provided.
* The subject_test and subject_train data sets contain the same number of rows as the x_test and x_train data sets, each row in subject_test and subject_train corresponds to the same number row in x_test and x_train.
* Read the subject_test and subject_train data sets into data frames.
* Each is located in its own subdirectory so use the pre-set subdirectory values to locate the files.
* Column bind the data sets to x_test and x_train.
* Set the new column name to 'Subject'.

**step 6 - merge the test and training data sets into one data frame**
* Having assembled the test and training data frames.
* Now merge the data sets into a single data frame: X_data, holding the combined set.

**step 7 drop any non-mean or non-standard deviation measure columns**
* The exercise asks for an extract of just the columns that hold mean and standard deviation values.
* To find which columns to keep:
    + use the features vector previously loaded, this holds the names of each measurement column
    + some of these hold text fragments 'mean' or 'std' - these are the columns we should keep
    + build a logical vector ('keep') the same length as the features vector
    + store TRUE in keep if the feature includes the text fragments 'mean' or 'std', otherwise store FALSE - look for this text within the feature names using the grepl() function
* Use the keep vector to drop any columns we don't need from x_data
* Note that columns 1-2 contain subject and activity data and are retained, so we explicity set keep[1:2] as TRUE
* This reduces the 561 measurement columns down to 87.

**step 8 - melt the data set, reduce 87 measurement columns to a variable and value column**
* Each row contains 87 measures plus a subject and activity column. 
* One row is in place for each measurement. 
* To tidy the data further we will reduce the 87 measures into two columns - 'variable' and 'value' using the melt() function
* There is an additional piece of data we need to keep - each row is created as a result of a single observation, all 87 measures are made at the same time and they are linked in some way, they are not totally independent of each other
* The raw data set does not explicity hold an observation id, this is implicit in the row number
* To retain information about the observation we need to add a further column to x_data, an index, before using melt()
    + create an integer vector from 1 to the number of rows in x_data
    + column bind the vector to x_data and name the column 'observation_number'
* Use melt to reduce the data set to 5 columns, observation_id, subject, activity, variable, value.
* Store the result in 'x_data_melt', a data frame.

**step 9 - calculate means for each measure by subject and activity**
* The dcast() function works with melt() data sets to calculate this data.
* Make 'Subject' and 'Activity' indexes of the calculation.
* Call dcast() with the function 'mean' and store the result in 'x_cast' data frame.
* 

**step 10 - print the tidy data set** 
* Print the results to a file "tidy_data.txt" using write.table()
* Use the row.name = FALSE setting

**step 11 - restore the environment**
* Reset the working directory to where it was when the script started.


## Description of Variables

* tBodyAcc-mean-X
* tBodyAcc-mean-Y
* tBodyAcc-mean-Z
* tBodyAcc-std-X
* tBodyAcc-std-Y
* tBodyAcc-std-Z

* tGravityAcc-mean-X
* tGravityAcc-mean-Y
* tGravityAcc-mean-Z
* tGravityAcc-std-X
* tGravityAcc-std-Y
* tGravityAcc-std-Z

* tBodyAccJerk-mean-X
* tBodyAccJerk-mean-Y
* tBodyAccJerk-mean-Z
* tBodyAccJerk-std-X
* tBodyAccJerk-std-Y
* tBodyAccJerk-std-Z

* tBodyGyro-mean-X
* tBodyGyro-mean-Y
* tBodyGyro-mean-Z
* tBodyGyro-std-X
* tBodyGyro-std-Y
* tBodyGyro-std-Z

* tBodyGyroJerk-mean-X
* tBodyGyroJerk-mean-Y
* tBodyGyroJerk-mean-Z
* tBodyGyroJerk-std-X
* tBodyGyroJerk-std-Y
* tBodyGyroJerk-std-Z

* tBodyAccMag-mean
* tBodyAccMag-std
* tGravityAccMag-mean
* tGravityAccMag-std

* tBodyAccJerkMag-mean
* tBodyAccJerkMag-std

* tBodyGyroMag-mean
* tBodyGyroMag-std

* tBodyGyroJerkMag-mean
* tBodyGyroJerkMag-std

* fBodyAcc-mean-X
* fBodyAcc-mean-Y
* fBodyAcc-mean-Z
* fBodyAcc-std-X
* fBodyAcc-std-Y
* fBodyAcc-std-Z

* fBodyAcc-meanFreq-X
* fBodyAcc-meanFreq-Y
* fBodyAcc-meanFreq-Z

* fBodyAccJerk-mean-X
* fBodyAccJerk-mean-Y
* fBodyAccJerk-mean-Z

* fBodyAccJerk-std-X
* fBodyAccJerk-std-Y
* fBodyAccJerk-std-Z
* fBodyAccJerk-meanFreq-X
* fBodyAccJerk-meanFreq-Y
* fBodyAccJerk-meanFreq-Z

* fBodyGyro-mean-X
* fBodyGyro-mean-Y
* fBodyGyro-mean-Z
* fBodyGyro-std-X
* fBodyGyro-std-Y
* fBodyGyro-std-Z
* fBodyGyro-meanFreq-X
* fBodyGyro-meanFreq-Y
* fBodyGyro-meanFreq-Z
* fBodyAccMag-mean
* fBodyAccMag-std
* fBodyAccMag-meanFreq
* fBodyBodyAccJerkMag-mean
* fBodyBodyAccJerkMag-std
* fBodyBodyAccJerkMag-meanFreq
* fBodyBodyGyroMag-mean
* fBodyBodyGyroMag-std
* fBodyBodyGyroMag-meanFreq
* fBodyBodyGyroJerkMag-mean
* fBodyBodyGyroJerkMag-std
* fBodyBodyGyroJerkMag-meanFreq
* angletBodyAccJerkMean,gravityMean
* angletBodyGyroMean,gravityMean
* angletBodyGyroJerkMean,gravityMean
* angleX,gravityMean
* angleY,gravityMean
* angleZ,gravityMean

## Sources

1. Tidy Data, Hadley Wickham, Journal of Statistical Software




