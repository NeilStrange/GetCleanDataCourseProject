README.Rmd
# Course Project, Getting and Cleaning Data
A description of how the script "run_analysis.R" works.
***
## Project Description
This project prepares a tidy data set, from a specific set of input data that contains smartphone accelerometer data from 30 individuals.  

**author**: Neil Strange  

**date**: 21.Sep.2015  

**input**: A subdirectory of source data: "UCI HAR Dataset" should be in the current working directory, containing accelerometer data from a number of subjects. A zip of this data is available from https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip.  

**output**: One tidy data file is produced - for each subject and activity combination a list of the mean of each measurement.  

## Package Dependencies
This script loads and uses the following additional packages:  
* dplyr, 0.4.3 or later
* data.table, 1.9.4 or later 
* reshape2, 1.4.1 or later  

## Interpretation of 'Tidy Data' for this Project
Tidy data is data that has been prepared for downstream anaysis, it makes it easy for an analyst to load and process the data. Hadley Wickham defines tidy data as having:
* each variable forms a column
* each observation forms a row
* each type of observational unit forms a table

In addition, columns should be named and rows may be named if appropriate.

For this project, the following rules have been applied:

* *Columns are named and the names are meaningful*: the data is scientific, the measurement names provided in the features file are appropriate for scientific use and are more meaningful than the original column names V1, V2... V561.  

* *Coded Values are Substituted for codes*: where activity codes 1..6 were used these are replaced with the corresponding strings set out in the activity_labels file, where 1 = WALKING, 2 = WALKING_UPSTAIRS, etc.  

* *There is one measure per row*: the original data contains multiple measure columns (561 columns of measures, reduced to 85 of interest - mean or standard deviation), these can be reduced to two columns: variable and value, note that - in order not to lose the fact that a number of measures were made at the same time, an extra column is added 'observation number' before melting the data set to retain the linkage between different observed measures.

## Data Processing
### Raw Source Data
Source data is contained in a directory structure which should be in the script's working directory when launched. This directory should have the name: "UCI HAR Dataset".  

The structure includes a number of subdirectories and files:  
* *\test* - a directory containing the test data set (see below)
* *\train* - a directory containing the training data set (see below)
* *activity_labels.txt* - a list of labels used to describe activities, activity data is stored in the test/training data sets using an integer number (1-6), this file contains the corresponding lookup values, i.e. a value of 1 is a 'WALKING' activity. The assignment asks these labels to be substituted for the integers in the target tidy data set.
* *features.txt* - a list of 561 measurements made by the sensor
* *features_info.txt* - a description of how the measurements were made and how they are structured
* *README.txt* - a description of the experimental source of the data, how measurements were made, the directories and files that comprise the data set and information about licensing 

Measurements were made for a number of subjects. The measures were divided into two sets - a test data set and a training data set, each set has its own subdirectory - \test and \train, and they contain further subdirectories and files. The \test and \train directories have identical structures, the table below describes the \test directory contents, however it is applicable to \train as well, just substitute file names using '_test' with '_train':  
* *X_test* - contains measurements or calculations based on these measurements, one row per observation event, each row with 561 observations labelled V1, V2, ...V561
* *y_test* - contains information about activities performed, as integers 1-6 (see features.txt to decode these), there is one entry in y_test for each entry in X_test and these correspond - so the first entry in y_test is the activity underway for the first measurement set in X_test
* *subject_test* - contains information about the subject being measured, identified as an integer, there is one entry in subject_test for each entry in X_test and these correspond - so the first entry in subject_test is the subject of the first measurement set in X_test
* *\Inertial Signals* - a subdirectory holding low-level body accelerator and body gyro data over a number of files, this is too low a level for use in this project and can be ignored  

### Target Data
The script produces a file 'tidy_data.txt' as output. This is a print out of a data frame 'x_tidy_data' holding the results of the tidy operations. In addition, the script creates a more detailed data frame, x_data_melt that holds the lower level data set from which x_tidy_data is calculated. Both x_tidy_data and x_data_melt are available in the environment for the user on completion of the script.

**x_tidy_data / tidy_data.txt**

Is a summary data set holding the mean of each measurement's values for each subject and activity combination, x_tidy_data is printed by the script as a tidy_data.txt file. 

Formatted as a data frame, 88 columns wide, 180 rows (all combinations of 6 activities x 30 subjects) with the following columns:
* Subject: integer, in the range 1:30, identifying the person who was the subject of the measurement
* Activity, character, one of 6 values (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)
* Columns 3:88, numeric, each column contains data about one variable, the data is the mean of all values for that variable for each subject and activity combination (see the code book for details of these columns).

**x_data_melt**

Is a narrow tidy data set holding lower level data for all mean and standard deviation measurements, tidied up from the original data set. 

x_data_melt is a data frame, 5 columns wide, with 885,174 rows.

It has the following columns:
* observation_number: integer, the row number from the original merged training and test data set, it ties together each of the 86 measurements made in a single observation
* Subject: integer, in the range 1:30, identifying the person who was the subject of the measurement
* Activity, character, one of 6 values (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)
* variable: character, the name of a variable, one of 86 measurements (see below for the list of variables)
* value: numeric, the value of that variable, all values are in the range -1 to +1, having been normalised


## Script: Processing Steps
The script contains a number of steps:  

**step 1 - initial preparation**
* This step verifies the working directory contains the required data set and stops with an error message if not.
* It saves the current working directory so it can be reset on exit.
* It sets some variables that store the subdirectory location for the test, train and features data sets.

**step 2 - read in the X_test/train data files**  
* Read in the files as comma separated data sets.
* Each is located in its own subdirectory so use the pre-set subdirectory values to locate the files.
* Keep them in separate data frames for processing before they are merged.
* The data frames are x_test and x_train.

**step 3 - rename the x_test/x_train data frame columns to use more readable names**  
* The raw data uses column names 'V1', V2', ... 'V561'. These do not explain what the columns represent.
* The features file contains a mapping of column names suitable for an engineer to use, e.g. tBodyAcc-mean-X, meaning a measure of mean body acceleration in the X direction measured over a time frame.
* Replace the column names with the set of names taken from the features.txt file.
* Read in the features file as a vector, storing what will become the new column names.
* Pre-process the features vector to remove (), as these are not suitable for column names, also convert '-' and ',' characters to '_'.
* Use the setnames() function to set both the x_test and x_train data frame column names to the features vector.

**step 4 - append the y_test/y_train activity data and relace codes with activity descriptions**  
* The y_test and Y_train files contain information about the activities being performed when the X-test and x_train measures were taken.
* The y_test and y_train data sets contain the same number of rows as the x_test and x_train data sets, each row in y_test and y_train corresponds to the same number row in x_test and x_train.
* Read the y_test and y_train data sets into data frames.
* Each is located in its own subdirectory so use the pre-set subdirectory values to locate the files.
* The raw activity data uses a numeric code 1:6.
* There are textual explanations of each code (in the activity_labels.txt file), 1 = WALKING, 2 = WALKING_UPSTAIRS, etc.
* Replace the raw activity codes with the activity labels to make them readable:
    + assemble the code explanations in its own vector - activity_code
    + loop through y_test and y_train replacing each code with its explanation value
    + hold the results in new vectors y_test_activities and y_train_activities
* Create new columns in x_test and x_train by column binding the y_test_activities and y_train_activities data
* name the new columns 'Activity'

**step 5 - append the subject_test and subject_train data**
* The subject_test and subject_train files contain information about subjects(people) involved in the experiment.
* Subjects are numbered 1:30, we don't know any more about them from the data provided.
* The subject_test and subject_train data sets contain the same number of rows as the x_test and x_train data sets, each row in subject_test and subject_train corresponds to the same number row in x_test and x_train.
* Read the subject_test and subject_train data sets into data frames.
* Each is located in its own subdirectory so use the pre-set subdirectory values to locate the files.
* Column bind the data sets to x_test and x_train.
* Set the new column name to 'Subject'.

**step 6 - merge the test and training data sets into one data frame**
* Having assembled the test and training data frames.
* Now merge the data sets into a single data frame: X_data, holding the combined set.

**step 7 - drop any non-mean or non-standard deviation measure columns**
* The exercise asks for an extract of just the columns that hold mean and standard deviation values.
* To find which columns to keep:
    + use the features vector previously loaded, this holds the names of each measurement column
    + some of these hold text fragments 'mean' or 'std' - these are the columns we should keep
    + build a logical vector ('keep') the same length as the features vector
    + store TRUE in keep if the feature includes the text fragments 'mean' or 'std', otherwise store FALSE - look for this text within the feature names using the grepl() function
* Use the keep vector to drop any columns we don't need from x_data
* Note that columns 1-2 contain subject and activity data and are retained, so we explicity set keep[1:2] as TRUE
* This reduces the 561 measurement columns down to 87.

**step 8 - melt the data set, reduce 87 measurement columns to a variable and value column**
* Each row contains 87 measures plus a subject and activity column. 
* One row is in place for each measurement. 
* To tidy the data further we will reduce the 87 measures into two columns - 'variable' and 'value' using the melt() function
* There is an additional piece of data we need to keep - each row is created as a result of a single observation, all 87 measures are made at the same time and they are linked in some way, they are not totally independent of each other
* The raw data set does not explicity hold an observation id, this is implicit in the row number
* To retain information about the observation we need to add a further column to x_data, an index, before using melt()
    + create an integer vector from 1 to the number of rows in x_data
    + column bind the vector to x_data and name the column 'observation_number'
* Use melt to reduce the data set to 5 columns, observation_id, subject, activity, variable, value.
* Store the result in 'x_data_melt', a data frame.

**step 9 - calculate means for each measure by subject and activity**
* The dcast() function works with melt() data sets to calculate this data.
* Make 'Subject' and 'Activity' indexes of the calculation.
* Call dcast() with the function 'mean' and store the result in 'x_tidy_data' data frame.
* 

**step 10 - print the tidy data set** 
* Print the x_tidy_data results to a file "tidy_data.txt" using write.table()
* Use the row.name = FALSE setting

**step 11 - restore the environment**
* Reset the working directory to where it was when the script started.


## Description of Variable Columns
Variable columns are present in the output "tidy_data.txt", and script data frames "tidy_data" and x_data_melt. These are tidied up input measures that contain mean and standard deviation data, i.e. any variable name containing the text fragments "mean" or "std". There are 86 mean and standard deviation variables in total.

tidy_data uses the variable names across columns 3:88, x_data_melt was created by applying the melt() function to reduce the variable set to two columns - 'variable' and 'value' holding the variable name and corresponding measure. One row in tidy_data will correspond to 86 rows in x_data_melt.

The variables are described in more detail in the **Code Book**, however they include mean and standard deviation (Std) data in x, y, and z coordinate directions for:

* Body Acceleration data(columns 3:8):
   + tBodyAcc-mean-X
   + tBodyAcc-mean-Y
   + tBodyAcc-mean-Z
   + tBodyAcc-std-X
   + tBodyAcc-std-Y
   + tBodyAcc-std-Z

* Gravity acceleration data(columns 9:14):   
   + tGravityAcc-mean-X
   + tGravityAcc-mean-Y
   + tGravityAcc-mean-Z
   + tGravityAcc-std-X
   + tGravityAcc-std-Y
   + tGravityAcc-std-Z

* Body jerk acceleration data(columns 15:20):
   + tBodyAccJerk-mean-X
   + tBodyAccJerk-mean-Y
   + tBodyAccJerk-mean-Z
   + tBodyAccJerk-std-X
   + tBodyAccJerk-std-Y
   + tBodyAccJerk-std-Z

* Body gyro-meaurements data(columns 21:26):
   + tBodyGyro-mean-X
   + tBodyGyro-mean-Y
   + tBodyGyro-mean-Z
   + tBodyGyro-std-X
   + tBodyGyro-std-Y
   + tBodyGyro-std-Z

* Body gyro-jerk measurements over a time frame(columns 27:32):
   + tBodyGyroJerk-mean-X
   + tBodyGyroJerk-mean-Y
   + tBodyGyroJerk-mean-Z
   + tBodyGyroJerk-std-X
   + tBodyGyroJerk-std-Y
   + tBodyGyroJerk-std-Z

* Body acceleration magnitude (Euclidean norm of x, y, z measures) (columns 33:36):
   + tBodyAccMag-mean
   + tBodyAccMag-std
   + tGravityAccMag-mean
   + tGravityAccMag-std

* Body jerk magnitude (Euclidean norm of x, y, z measures) (columns 37:38):
   + tBodyAccJerkMag-mean
   + tBodyAccJerkMag-std

* Body gyro magnitude (Euclidean norm of x, y, z measures)(columns 39:40):
   + tBodyGyroMag-mean
   + tBodyGyroMag-std

* Body gyro jerk magnitude (Euclidean norm of x, y, z measures)(columns 41:42):
   + tBodyGyroJerkMag-mean
   + tBodyGyroJerkMag-std

* Fourier transform of body acceleration(columns 43:48):
   + fBodyAcc-mean-X
   + fBodyAcc-mean-Y
   + fBodyAcc-mean-Z
   + fBodyAcc-std-X
   + fBodyAcc-std-Y
   + fBodyAcc-std-Z

* Fourier transform of body acceleration, mean frequency(columns 49:51):
   + fBodyAcc-meanFreq-X
   + fBodyAcc-meanFreq-Y
   + fBodyAcc-meanFreq-Z

* Fourier transform of body acceleration jerk(columns 52:57):
   + fBodyAccJerk-mean-X
   + fBodyAccJerk-mean-Y
   + fBodyAccJerk-mean-Z
   + fBodyAccJerk-std-X
   + fBodyAccJerk-std-Y
   + fBodyAccJerk-std-Z

* Fourier transform of body acceleration jerk, mean frequency(columns 58:60):
   + fBodyAccJerk-meanFreq-X
   + fBodyAccJerk-meanFreq-Y
   + fBodyAccJerk-meanFreq-Z

* Fourier transform of body gyro (columns 61:66):
   + fBodyGyro-mean-X
   + fBodyGyro-mean-Y
   + fBodyGyro-mean-Z
   + fBodyGyro-std-X
   + fBodyGyro-std-Y
   + fBodyGyro-std-Z

* Fourier transform of body gyro, mean frequency (columns 67:69):
   + fBodyGyro-meanFreq-X
   + fBodyGyro-meanFreq-Y
   + fBodyGyro-meanFreq-Z

* Fourier transform of body acceleration magnitude (columns 70:71):
   + fBodyAccMag-mean
   + fBodyAccMag-std

* Fourier transform of body acceleration magnitude, mean frequency (column 72):
   + fBodyAccMag-meanFreq

* Fourier transform of body acceleration jerk magnitude (columns 73:75):
   + fBodyBodyAccJerkMag-mean
   + fBodyBodyAccJerkMag-std
   + fBodyBodyAccJerkMag-meanFreq

* Fourier transform of body gyro jerk magnitude (columns 76:77):
   + fBodyBodyGyroMag-mean
   + fBodyBodyGyroMag-std

* Fourier transform of body gyro magnitude, mean frequency (column 78):
   + fBodyBodyGyroMag-meanFreq

* Fourier transform of body gyro jerk magnitude (columns 79:80):
   + fBodyBodyGyroJerkMag-mean
   + fBodyBodyGyroJerkMag-std

* Fourier transform of body gyro jerk magnitude, mean frequency (column 81):
   + fBodyBodyGyroJerkMag-meanFreq

* Body acceleration gyro, jerk and gravity mean(columns 82:84):
   + angletBodyAccJerkMean,gravityMean
   + angletBodyGyroMean,gravityMean
   + angletBodyGyroJerkMean,gravityMean

* Angle gravity mean(columns 85:87):
   + angleX,gravityMean
   + angleY,gravityMean
   + angleZ,gravityMean

## Sources

1. Tidy Data, Hadley Wickham, Journal of Statistical Software




